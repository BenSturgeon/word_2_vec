{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d26427",
   "metadata": {},
   "source": [
    "# Goals of this project\n",
    "The aim of this project was to act as a simple exploratory project to practice building a fairly fundamental tool in NLP.\n",
    "\n",
    "I learned the concepts behind the word2vec model, and while it was fairly understandable I wanted to see how it would translate to code.\n",
    "\n",
    "I also got to practice working more with the pytorch library as a result, which was a big win.\n",
    "\n",
    "The biggest challenge for me in building this was getting the vector dimensions right for matrix multiplication. Learning to respect that process and approach it slowly was valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3df08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from fastcore import *\n",
    "from nbdev.showdoc import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bdd0ad1",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "Here we define some functions for reading in and cleaning the data.\n",
    "\n",
    "The primary data cleaning involves removing any characters which aren't letters as well making all those letters lowercase. \n",
    "We also remove all whitespace except for single spaces between words.\n",
    "\n",
    "We want to remove words which do not carry much semantic weight on their own, which we will call stop words. We simply filter these out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "def remove_non_alpha_characters(data):\n",
    "    data = data.lower()\n",
    "    # use regex to remove all non-alphanumeric characters\n",
    "    data = re.sub(r'[^a-zA-Z\\s]', '', data)\n",
    "    # use regex to remove all whitespace characters\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stopwords = ['a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'when', 'at', 'from', 'by', 'on', 'off', 'for', 'in', 'out', 'over', 'to', 'into', 'with', \"\"]\n",
    "    data = [word for word in data if word not in stopwords]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d33ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_file('data/shakespeare.txt')\n",
    "\n",
    "data = remove_non_alpha_characters(raw_data)\n",
    "data = data.split(\" \")\n",
    "data = remove_stopwords(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c69e06b",
   "metadata": {},
   "source": [
    "Here we create our dictionary of unique words which we will use for defining our embeddings.\n",
    "\n",
    "We make this into a dictionary so that later we can refer to these values and get an index from each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bda534",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(list(data))\n",
    "unique_dict = {word: i for i, word in enumerate(unique_words)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0990013b",
   "metadata": {},
   "source": [
    "### Create a train loader and dataset\n",
    "Here we create a train_loader that will randomly generate samples for us to use during training.\n",
    "\n",
    "We choose a large batch size as this task does not demand a great deal of memory and larger batches help with training speed.\n",
    "\n",
    "We define only the functions here and call them later so that all hyper parameters can be defined and run in one place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_without_a_value(data, value):\n",
    "    return [x for x in data if x != value]\n",
    "\n",
    "def create_dataset(window_size, data):\n",
    "    dataset = []\n",
    "\n",
    "    for index, val in enumerate(data):\n",
    "        sub = data[max(0,index-window_size):index]\n",
    "        sub.extend(data[index+1:min(index+window_size, len(data))])\n",
    "        for target in sub:\n",
    "            dataset.append((unique_dict[val],unique_dict[target]))\n",
    "    return dataset  \n",
    "\n",
    "def create_dataloader(dataset, batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2074e36",
   "metadata": {},
   "source": [
    "## Defining our model\n",
    "\n",
    "Here we define our skipgram model class.\n",
    "\n",
    "We take advantage of the built in pytorch module and embedding class to create our vector embeddings and handle the updating of our embedding weights.\n",
    "\n",
    "Our embeddings are essentially just matrices with dimensions matching the size of our dictionary and the embedding size we choose to represent the different features that will emerge from our data.\n",
    "\n",
    "One thing to take note of is how we initialise our weights. Currently we simply use a normal distribution with a mean of 0 and std_deviation of 0.1. It is possible there are better initialisations to use here.\n",
    "We also use sparse embeddings many of the values will be very close to, or at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9401026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        init_mean = 0\n",
    "        init_std = 0.1\n",
    "        self.u_embeddings.weight.data.normal_(init_mean, init_std)\n",
    "        self.v_embeddings.weight.data.normal_(init_mean, init_std)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u).view(-1, 1, self.embedding_dim).squeeze()\n",
    "        emb_v = self.v_embeddings(pos_v).view(-1, self.embedding_dim).squeeze()\n",
    "\n",
    "        score = torch.bmm(emb_u.unsqueeze(1), emb_v.unsqueeze(2)).squeeze()\n",
    "        score = torch.sigmoid(score)\n",
    "\n",
    "        neg_emb_v = self.v_embeddings(neg_v).view(-1, self.embedding_dim, neg_v.shape[1])\n",
    "\n",
    "        neg_score = torch.bmm(emb_u.unsqueeze(1), neg_emb_v).squeeze()\n",
    "        neg_score = torch.sigmoid(neg_score)\n",
    "\n",
    "        return score, neg_score\n",
    "\n",
    "\n",
    "    \n",
    "    def forward_without_negatives(self, word1, word2):\n",
    "        \"\"\"This lets us do a simple vector comparison rather than doing a full forward pass.\"\"\"\"\"\n",
    "        pos_u = torch.tensor([unique_dict[word1]])\n",
    "        pos_v = torch.tensor([unique_dict[word2]])\n",
    "        emb_u = self.u_embeddings(pos_u).view(-1, 1, self.embedding_dim).squeeze()\n",
    "        emb_v = self.v_embeddings(pos_v).view(-1, self.embedding_dim).squeeze()\n",
    "        score = torch.dot(emb_u, emb_v)\n",
    "        score = torch.sigmoid(score)\n",
    "        return score\n",
    "\n",
    "    def get_dict_embeddings(self):\n",
    "        return self.u_embeddings.weight.data.cpu().numpy()\n",
    "    \n",
    "    def get_embedding_from_word(self, word):\n",
    "        index = unique_dict[word]\n",
    "        return self.u_embeddings.weight.data[index]\n",
    "    \n",
    "    def get_embedding_from_index(self, index):\n",
    "        return self.u_embeddings.weight.data[index]\n",
    "\n",
    "    def save_embedding(self, file_name):\n",
    "        # Save embedding lookup table as pkl file\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pkl.dump(self.u_embeddings.weight.data.cpu().numpy(), f)\n",
    "    \n",
    "    def import_embeddings(self, file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.u_embeddings.weight.data = torch.from_numpy(pkl.load(f)).to(torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5702ad3",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "Here we create a custom loss function which gives us a loss based on the model's error when predicting a 1 or 0 for the context word or randomly sampled words.\n",
    "\n",
    "We use a custom loss function because it allows us to add weight decay to our training and capture the specific nature of what we want the model to improve at, which in this case is relatedness of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f32bc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(score, neg_score, lr, weight_decay, model):\n",
    "    pos_loss = -torch.mean(torch.log(score))\n",
    "    neg_loss = -torch.mean(torch.sum(torch.log(1 - neg_score), dim=1))\n",
    "    loss = pos_loss + neg_loss\n",
    "    # add L2 regularization term\n",
    "    l2_loss = 0\n",
    "    for param in model.parameters():\n",
    "        l2_loss += torch.sum(param**2)\n",
    "        loss += weight_decay * l2_loss\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d9d29b8",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "This is the training loop for our model.\n",
    "\n",
    "Our train loader iterator is declared every epoch and we then iterate over it according to our steps per epoch.\n",
    "\n",
    "We generate our negative samples randomly at runtime as the cost of doing so is very low.\n",
    "\n",
    "The parameters passed in for training have a massive impact on model performance. \n",
    "The length of negative samples should be somewhere between 5 and 20. Having a lower numbers means the model may stray into simply having all of its values tend to 1 which is not what we want. Thus a higher number is favoured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ecdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, batch_size, negative_sample_length, weight_decay, learning_rate, steps_per_epoch, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        train_loader_iter = iter(train_loader)\n",
    "        for i in tqdm(range(steps_per_epoch)):\n",
    "            x, y = next(train_loader_iter)\n",
    "            pos_u = torch.tensor(x)\n",
    "            pos_v = torch.tensor(y)\n",
    "            neg_v = torch.randint(0, dictionary_length, (batch_size, negative_sample_length))\n",
    "            optimizer.zero_grad()\n",
    "            pos_score, neg_score = model(pos_u, pos_v, neg_v)\n",
    "            loss = loss_function(pos_score, neg_score, learning_rate, weight_decay, model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "        print(\"Epoch: {}, Loss: {}\".format(epoch, loss_sum / steps_per_epoch))\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac1711eb",
   "metadata": {},
   "source": [
    "## Instantiating our model\n",
    "\n",
    "Here we instantiate our model with the embedding dimensions of our choice. \n",
    "\n",
    "The embedding dimensions are critical because they dictate how many \"features\" our vectors can contain. More dimensions mean richer vectors, with the trade off being longer trading times.\n",
    "\n",
    "I find that 100 is a good number for getting sensible results on vector comparison, but even having very few can produce decent results.\n",
    "\n",
    "I instantiate this in a different step than running the training loop as I don't necessarily want to throw away my weights every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "dictionary_length = len(unique_words)\n",
    "model = SkipGramModel(dictionary_length, embedding_dim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "809e9c04",
   "metadata": {},
   "source": [
    "# Training hyperparameters\n",
    "\n",
    "The key hyperparameters we choose here are our window size, negative sample length, learning rate and weight decay.\n",
    "\n",
    "* Window size: Determines the size of the context we will include for each of our unique words. We will take in x words from the left and right. Having a smaller window size(2-15) gives us a sense of the interchangeability of the words, while a larger window size (15 - 50) gives us a sense of the relatedness of the words.\n",
    "\n",
    "* Negative sample length: To make the classifation task difficult for our model and avoid having it giving a score of 1 for relatedness on all words we need to have negative samples to make the problem harder. To get these samples we simply randomly sample from our dataset of unique words. The number of negative samples we use in each batch determines how many negative samples will be used for generating our negative score.\n",
    "\n",
    "* Learning rate: The degree to which we update our weights with each pass. In this case we want a fairly low learning rate as higher rates tend to cause the model to not converge.\n",
    "\n",
    "* Weight decay: Having some regularisation is important for this task as otherwise the weights tend to converge on very similar values. To avoid this we have the weights naturally decay over time. Small values seem to work for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbe187",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "negative_sample_length = 15\n",
    "weight_decay = 0.0006\n",
    "steps_per_epoch = 300\n",
    "epochs = 30\n",
    "\n",
    "dataset = create_dataset(window_size , data) \n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "train(model, train_loader, batch_size, negative_sample_length, weight_decay, learning_rate, steps_per_epoch, epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "011b5ae7",
   "metadata": {},
   "source": [
    "# Testing\n",
    "We now go to the testing phase to see how our model is performing.\n",
    "\n",
    "### Testing functions\n",
    "The following functions primarily exist to add, subtract and compare vectors. The goal is to produce intuitive results from the comparisons of our vectors.\n",
    "\n",
    "Eg the following should have a high correlation:\n",
    "flower and rose\n",
    "man and king\n",
    "\n",
    "man and woman\n",
    "\n",
    "queen and woman\n",
    "#### The following should have a low correlation\n",
    "Flower and cart\n",
    "\n",
    "concept and dog\n",
    "\n",
    "power and table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def cos_sim_word(word1, word2):\n",
    "    vector1 = get_emb(word1)\n",
    "    vector2 = get_emb(word2)\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def get_emb(word):\n",
    "    return model.get_embedding_from_word(word)\n",
    "\n",
    "def invert_dictionary(dictionary):\n",
    "    return {v: k for k, v in dictionary.items()}\n",
    "\n",
    "def get_closest_vector(vector):\n",
    "    max = 0\n",
    "    target = None\n",
    "    for key,item in unique_dict.items():\n",
    "        comparative = get_emb(key)\n",
    "        comparison = cos_sim(vector, comparative)\n",
    "        if comparison > max:\n",
    "            max = comparison\n",
    "            target = key\n",
    "\n",
    "        \n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b692c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cos_sim_word(\"flower\", \"rose\"),(\"flower\", \"rose\"))\n",
    "print(cos_sim_word(\"flower\", \"tree\"), (\"flower\", \"tree\"))\n",
    "print(cos_sim_word(\"flower\", \"dog\"), (\"flower\", \"dog\"))\n",
    "print(cos_sim_word(\"flower\", \"metal\"), (\"flower\", \"metal\"))\n",
    "print(cos_sim_word(\"flower\", \"cart\"), (\"flower\", \"cart\"))\n",
    "print(cos_sim_word(\"worm\", \"dog\"), (\"worm\", \"dog\"))\n",
    "print(cos_sim_word(\"king\", \"queen\"), (\"king\", \"queen\"))\n",
    "print(cos_sim_word(\"king\", \"royalty\"), (\"king\", \"royalty\"))\n",
    "print(cos_sim_word(\"queen\", \"royalty\"), (\"queen\", \"royalty\"))\n",
    "print(cos_sim_word(\"man\", \"king\"), (\"man\", \"king\"))\n",
    "print(cos_sim_word(\"woman\", \"king\"), (\"woman\", \"king\"))\n",
    "print(cos_sim_word(\"child\", \"prince\"), (\"child\", \"prince\"))\n",
    "print(cos_sim_word(\"child\", \"thought\"), (\"child\", \"thought\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f77bc95",
   "metadata": {},
   "source": [
    "This classic test still doesn't seem to work great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_emb(\"king\") - get_emb(\"man\") + get_emb(\"woman\") \n",
    "closest_vector = get_closest_vector(vector)\n",
    "print(closest_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6654677",
   "metadata": {},
   "source": [
    "Here we have the option to generate some common words. Currently I prefer using a small hand picked set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_most_common_words(words, n, m):\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = 0\n",
    "        word_counts[word] += 1\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_words = [word[0] for word in sorted_words]\n",
    "    return sorted_words[n:m]\n",
    "\n",
    "common_words = find_most_common_words(data, 100, 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "700ad8e9",
   "metadata": {},
   "source": [
    "Save the embeddings using any filename you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"outputs/embeddings.emb\"\n",
    "model.save_embedding(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a6e3c82",
   "metadata": {},
   "source": [
    "Import the embeddings if you simply wish to display them without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.import_embeddings(\"embeddings.emb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb37e70b",
   "metadata": {},
   "source": [
    "Here we choose some possibly interesting words to look at in our dataset, based on my poor knowledge of Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = [\"king\",\"queen\", \"prince\", \"princess\", \"knight\",\"skull\", \"sword\", \"skull\", \"flower\", \"tree\", \"fool\", \"metal\", \"cart\", \"worm\", \"boy\", \"witch\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d2227e1",
   "metadata": {},
   "source": [
    "## Displaying our results\n",
    "Here we use principal component analysis to reduce our dimensions down to 2 so we can display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "number_components = 2\n",
    "\n",
    "embeddings = model.get_dict_embeddings()\n",
    "pca = PCA(n_components=number_components)\n",
    "reduced_embeddings= pca.fit_transform(embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd29d7a2",
   "metadata": {},
   "source": [
    "### Display the embeddings\n",
    "display embeddings visually using matplotlib. \n",
    "\n",
    "Each word is represented by a point in 2D space.\n",
    "\n",
    "The x and y coordinates of the point are the first and second dimensions of the word's embedding.\n",
    "\n",
    "The words are labeled by their actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadf5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for word in words:\n",
    "  coord = reduced_embeddings[unique_dict[word]]\n",
    "  plt.scatter(coord[0], coord[1])\n",
    "  plt.annotate(word, (coord[0], coord[1]))\n",
    "\n",
    "plt.savefig('outputs/embeddings.png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab74e5b4",
   "metadata": {},
   "source": [
    "#### The following is a way of displaying the vectors and their embeddings clearly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def plot_embeddings(words, number_dimensions, reduced_embeddings):\n",
    "  # Get the embeddings for the given list of words\n",
    "  embeddings = [reduced_embeddings[unique_dict[word]] for word in words]\n",
    "\n",
    "  # Create a list to store the table rows\n",
    "  rows = []\n",
    "\n",
    "  # Plot the first x embeddings\n",
    "  for word in words:\n",
    "    embedding = reduced_embeddings[unique_dict[word]]\n",
    "\n",
    "    # Convert the embedding array to a list and take the first 'number_dimensions' elements\n",
    "    embedding = embedding.tolist()\n",
    "    embedding = embedding[:number_dimensions]\n",
    "\n",
    "    # Create a row with the word and the embeddings, followed by the color for each embedding\n",
    "    row = [word] + [f\"{e:.2f}\" for e in embedding] \n",
    "    rows.append(row)\n",
    "\n",
    "  # Use IPython's display function to display the table\n",
    "  display(HTML(\n",
    "      '<table><tr>{}</tr></table>'.format(\n",
    "          '</tr><tr>'.join(\n",
    "              '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in rows)\n",
    "          )\n",
    "  ))\n",
    "\n",
    "words = [\"king\",\"queen\"]\n",
    "plot_embeddings(words, 5, reduced_embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a780b12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The following has been an initial exploration of word embeddings.\n",
    "\n",
    "While I would like to revisit this kind of work in future and I am sure there are many improvements I could make there are other projects to move on to.\n",
    "\n",
    "Any feedback is very welcome at bwm.sturgeon@gmail.com\n",
    "\n",
    "I hope reading this has been a clear and useful insight into how to train a basic word2vec model using skipgram with negative sampling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wordvec",
   "language": "python",
   "name": "venv_wordvec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6a9f868315655f86619a29ce0a043959ee2de2105a58d548d1647254ae6395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b3df08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from fastcore import *\n",
    "from nbdev.showdoc import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0f6251",
   "metadata": {},
   "source": [
    "# Goals of this project\n",
    "The aim of this project was to act as a simple exploratory project to practice building a fairly fundamental tool in NLP.\n",
    "\n",
    "I learned the concepts behind the word2vec model, and while it was fairly understandable I wanted to see how it would translate to code.\n",
    "\n",
    "I also got to practice working more with the pytorch library as a result, which was a big win.\n",
    "\n",
    "The biggest challenge for me in building this was getting the vector dimensions right for matrix multiplication. Learning to respect that process and approach it slowly was valuable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07ecaaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "raw_data = read_file('shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ad8b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scalar_loss(pos_score, neg_score, criterion, concatenated_data):\n",
    "    \"\"\"function to get the scalar loss. Unused because the results are generally bad from my current experiments.\"\"\"\n",
    "    score = torch.cat([pos_score, neg_score.flatten()], dim=0)\n",
    "    combined_len = len(pos_score) + len(neg_score)\n",
    "    pos_u_data = torch.ones(len(pos_score), 1)\n",
    "    neg_v_data = torch.zeros(len(neg_score.flatten()), 1)\n",
    "    loss = criterion(score, concatenated_data)\n",
    "    return loss\n",
    "    loss = get_scalar_loss( pos_score, neg_score, criterion, concat_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a577ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### non_scalar_loss\n",
       "\n",
       ">      non_scalar_loss (score, neg_score, lr, weight_decay, model)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### non_scalar_loss\n",
       "\n",
       ">      non_scalar_loss (score, neg_score, lr, weight_decay, model)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(non_scalar_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d33ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_characters(data):\n",
    "    \"\"\"Remove whitespaces and non alpha characters from a string\"\"\"\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'[^a-zA-Z\\s]', '', data)\n",
    "    data = re.sub(r'\\s+', ' ', data)\n",
    "    return data\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stopwords = ['a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'when', 'at', 'from', 'by', 'on', 'off', 'for', 'in', 'out', 'over', 'to', 'into', 'with', \"\"]\n",
    "    data = [word for word in data if word not in stopwords]\n",
    "    return data\n",
    "\n",
    "data = remove_non_alpha_characters(raw_data)\n",
    "data = data.split(\" \")\n",
    "data = remove_stopwords(data)\n",
    "unique_words = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bda534",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dict = {word: i for i, word in enumerate(unique_words)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae497726",
   "metadata": {},
   "source": [
    "### Create a dictionary of words and the positions they appear in the data\n",
    "This is to speed up the time it takes to build our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58bc2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pos = {}\n",
    "for i, word in enumerate(data):\n",
    "    if word in word_pos:\n",
    "        word_pos[word].append(i)\n",
    "    else:\n",
    "        word_pos[word] = [i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "134d5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def return_list_without_a_value(list, value):\n",
    "    return [x for x in list if x != value]\n",
    "\n",
    "window_size = 5\n",
    "dataset = []\n",
    "sample_data = data\n",
    "\n",
    "for i, val in enumerate(sample_data):\n",
    "    if i > len(sample_data) - window_size:\n",
    "        break\n",
    "    sub = sample_data[i:i+window_size]\n",
    "    included = return_list_without_a_value(sub, val)\n",
    "    for target in included:\n",
    "        dataset.append((unique_dict[val],unique_dict[target]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "85a31fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30997it [03:17, 156.96it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_context_words(data, word, window_size):\n",
    "    context_words = []\n",
    "    for i in word_pos[word]:\n",
    "        \n",
    "        context_words.extend(data[i-window_size: i])\n",
    "        context_words.extend(data[i+1:i+window_size])\n",
    "\n",
    "\n",
    "    return context_words\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "dataset = np.array([0,0]).reshape(-1,2)\n",
    "sample_data = data\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i, val in tqdm(enumerate(unique_words)):\n",
    "    # if i > len(sample_data) - window_size:\n",
    "    #     break\n",
    "    context_words = get_context_words(sample_data, val, window_size)\n",
    "    \n",
    "    # using numpy broadcasting we create a numpy array with the unique_dict[val] as the first column and the context words as the second column\n",
    "    new_data = np.array([unique_dict[val]] * len(context_words)).reshape(-1,1)\n",
    "    new_data = np.append(new_data, np.array([unique_dict[word] for word in context_words]).reshape(-1,1), axis=1)\n",
    "    \n",
    "    dataset = np.append(dataset, new_data, axis=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e9ea484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "n_iters = 3000\n",
    "num_epochs = 100\n",
    "num_epochs = int(num_epochs)\n",
    "# create a train_loader that will randomly generate examples forever\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "14289bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27613, 22237],\n",
      "        [21709, 10357],\n",
      "        [21444, 20861],\n",
      "        [ 9227,  7180],\n",
      "        [ 7098, 30318],\n",
      "        [17800, 21128],\n",
      "        [24258, 24290],\n",
      "        [29306, 10357],\n",
      "        [25981,  5158],\n",
      "        [  879, 14039],\n",
      "        [29609, 10140],\n",
      "        [22891,  7996],\n",
      "        [19007, 10140],\n",
      "        [20335, 23987],\n",
      "        [ 3375, 14039]])\n",
      "tensor([[ 9964, 28049],\n",
      "        [14573,  2070],\n",
      "        [12195, 17800],\n",
      "        [15011, 14303],\n",
      "        [17266, 17885],\n",
      "        [ 3484, 13198],\n",
      "        [  704,  9789],\n",
      "        [ 3919,   998],\n",
      "        [ 4521,  6803],\n",
      "        [29280, 17800],\n",
      "        [20334,  9046],\n",
      "        [30318, 21221],\n",
      "        [24597, 28378],\n",
      "        [19007, 12011],\n",
      "        [30278, 28049]])\n",
      "tensor([[ 7036,  6102],\n",
      "        [ 7580, 17204],\n",
      "        [ 2709, 17290],\n",
      "        [ 7996, 14956],\n",
      "        [ 3135, 30178],\n",
      "        [24081,  2004],\n",
      "        [20335, 11720],\n",
      "        [ 3919, 10710],\n",
      "        [23698,   721],\n",
      "        [ 3919, 10140],\n",
      "        [14247, 28049],\n",
      "        [ 6299, 20737],\n",
      "        [10897, 28049],\n",
      "        [22891, 16795],\n",
      "        [ 2512,  3304]])\n",
      "tensor([[15039, 24522],\n",
      "        [30318,  8663],\n",
      "        [26298, 28049],\n",
      "        [ 2901, 24258],\n",
      "        [12672, 24191],\n",
      "        [13666,  7780],\n",
      "        [11304,  7454],\n",
      "        [10357,  3135],\n",
      "        [ 6076, 17106],\n",
      "        [26593,  7037],\n",
      "        [29689, 21676],\n",
      "        [10206, 28049],\n",
      "        [17800, 18138],\n",
      "        [11596, 14537],\n",
      "        [15706, 19235]])\n",
      "tensor([[17752, 16842],\n",
      "        [27204, 23301],\n",
      "        [29339,  5655],\n",
      "        [19007, 29889],\n",
      "        [21308, 26861],\n",
      "        [27696, 24411],\n",
      "        [ 2106, 19605],\n",
      "        [17800, 15511],\n",
      "        [ 8663,  5872],\n",
      "        [10357, 30278],\n",
      "        [16517,  2207],\n",
      "        [16331, 11904],\n",
      "        [13211, 16689],\n",
      "        [24830,  7431],\n",
      "        [25535,  4975]])\n",
      "tensor([[22969, 12641],\n",
      "        [ 7043, 24830],\n",
      "        [23840,  5388],\n",
      "        [22969,  6501],\n",
      "        [27745, 19125],\n",
      "        [29033, 24438],\n",
      "        [ 5098, 16795],\n",
      "        [ 3057, 19235],\n",
      "        [27632, 22918],\n",
      "        [19285, 23428],\n",
      "        [ 4086,  2376],\n",
      "        [30318, 19235],\n",
      "        [10140,  8778],\n",
      "        [27569,  4653],\n",
      "        [20258,   809]])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    count +=1 \n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f9401026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        init_mean = 0\n",
    "        init_std = 0.01\n",
    "        self.u_embeddings.weight.data.normal_(init_mean, init_std)\n",
    "        self.v_embeddings.weight.data.normal_(init_mean, init_std)\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.v_embeddings(pos_u).view(-1,1, self.embedding_dim).squeeze()\n",
    "        emb_v = self.v_embeddings(pos_v).view(-1, self.embedding_dim).squeeze()\n",
    "        score = torch.bmm(emb_u.unsqueeze(1), emb_v.unsqueeze(2)).squeeze()\n",
    "        score = torch.sigmoid(score)\n",
    "        neg_emb_v = self.v_embeddings(neg_v).view(-1, self.embedding_dim, neg_v.shape[1])\n",
    "        neg_score = torch.bmm(emb_u.unsqueeze(1), neg_emb_v).squeeze()\n",
    "        neg_score = torch.sigmoid(neg_score)\n",
    "        return score, neg_score\n",
    "    \n",
    "    def forward_without_negatives(self, word1, word2):\n",
    "        pos_u = torch.tensor([unique_dict[word1]])\n",
    "        pos_v = torch.tensor([unique_dict[word2]])\n",
    "        emb_u = self.u_embeddings(pos_u).view(-1, 1, self.embedding_dim).squeeze()\n",
    "        emb_v = self.v_embeddings(pos_v).view(-1, self.embedding_dim).squeeze()\n",
    "        score = torch.dot(emb_u, emb_v)\n",
    "        score = torch.sigmoid(score)\n",
    "        return score\n",
    "\n",
    "    def get_dict_embeddings(self):\n",
    "        return self.u_embeddings.weight.data.cpu().numpy()\n",
    "    \n",
    "    def get_embedding_from_word(self, word):\n",
    "        index = unique_dict[word]\n",
    "        return self.u_embeddings.weight.data[index]\n",
    "    \n",
    "    def get_embedding_from_index(self, index):\n",
    "        return self.u_embeddings.weight.data[index]\n",
    "\n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.u_embeddings\n",
    "        fout = open(file_name, 'w')\n",
    "        fout.write('{} {}\\n'.format(len(id2word), self.embedding_dim))\n",
    "        for wid, w in id2word.items():\n",
    "            e = ' '.join(map(lambda x: str(x), self.get_embedding_from_index(wid)))\n",
    "            fout.write('{} {}\\n'.format(w, e))\n",
    "        fout.close()\n",
    "    \n",
    "    def import_embeddings(self, file_name):\n",
    "        fin = open(file_name, 'r')\n",
    "        n, d = map(int, fin.readline().split())\n",
    "        embedding = np.zeros((n, d))\n",
    "        word2id = {}\n",
    "        for line in fin:\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word2id[tokens[0]] = len(word2id)\n",
    "            embedding[word2id[tokens[0]]] = list(map(float, tokens[1:]))\n",
    "        return embedding, word2id\n",
    "    \n",
    "    def non_scalar_loss(self, score, neg_score, lr, weight_decay):\n",
    "        pos_loss = -torch.mean(torch.log(score))\n",
    "        neg_loss = -torch.mean(torch.sum(torch.log(1 - neg_score), dim=1))\n",
    "        loss = pos_loss + neg_loss\n",
    "        # add L2 regularization term\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param**2)\n",
    "            loss += weight_decay * l2_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "  \n",
    "embedding_dim = 100\n",
    "window_size = 5\n",
    "\n",
    "dictionary_length = len(unique_words)\n",
    "\n",
    "model = SkipGramModel(dictionary_length, embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "324ecdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/3pxy4p9914ld14l6mxvvknvw0000gn/T/ipykernel_58059/3903209029.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_u = torch.tensor(val[:,0])\n",
      "/var/folders/xr/3pxy4p9914ld14l6mxvvknvw0000gn/T/ipykernel_58059/3903209029.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_v = torch.tensor(val[:,1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step [201/479987], Loss: 1.9303\n",
      " Step [401/479987], Loss: 1.8715\n",
      " Step [601/479987], Loss: 1.8735\n",
      " Step [801/479987], Loss: 1.8821\n",
      " Step [1001/479987], Loss: 1.8817\n",
      " Step [1201/479987], Loss: 1.8808\n",
      " Step [1401/479987], Loss: 1.8786\n",
      " Step [1601/479987], Loss: 1.8785\n",
      " Step [1801/479987], Loss: 1.8912\n",
      " Step [2001/479987], Loss: 1.8740\n",
      " Step [2201/479987], Loss: 1.8702\n",
      " Step [2401/479987], Loss: 1.8824\n",
      " Step [2601/479987], Loss: 1.8868\n",
      " Step [2801/479987], Loss: 1.8704\n",
      " Step [3001/479987], Loss: 1.8730\n",
      " Step [3201/479987], Loss: 1.8702\n",
      " Step [3401/479987], Loss: 1.8719\n",
      " Step [3601/479987], Loss: 1.8735\n",
      " Step [3801/479987], Loss: 1.8669\n",
      " Step [4001/479987], Loss: 1.8799\n",
      " Step [4201/479987], Loss: 1.8618\n",
      " Step [4401/479987], Loss: 1.8779\n",
      " Step [4601/479987], Loss: 1.8751\n",
      " Step [4801/479987], Loss: 1.8755\n",
      " Step [5001/479987], Loss: 1.8610\n",
      " Step [5201/479987], Loss: 1.8620\n",
      " Step [5401/479987], Loss: 1.8706\n",
      " Step [5601/479987], Loss: 1.8667\n",
      " Step [5801/479987], Loss: 1.8690\n",
      " Step [6001/479987], Loss: 1.8639\n",
      " Step [6201/479987], Loss: 1.8493\n",
      " Step [6401/479987], Loss: 1.8619\n",
      " Step [6601/479987], Loss: 1.8606\n",
      " Step [6801/479987], Loss: 1.8568\n",
      " Step [7001/479987], Loss: 1.8619\n",
      " Step [7201/479987], Loss: 1.8715\n",
      " Step [7401/479987], Loss: 1.8511\n",
      " Step [7601/479987], Loss: 1.8586\n",
      " Step [7801/479987], Loss: 1.8709\n",
      " Step [8001/479987], Loss: 1.8498\n",
      " Step [8201/479987], Loss: 1.8604\n",
      " Step [8401/479987], Loss: 1.8580\n",
      " Step [8601/479987], Loss: 1.8646\n",
      " Step [8801/479987], Loss: 1.8735\n",
      " Step [9001/479987], Loss: 1.8556\n",
      " Step [9201/479987], Loss: 1.8586\n",
      " Step [9401/479987], Loss: 1.8594\n",
      " Step [9601/479987], Loss: 1.8504\n",
      " Step [9801/479987], Loss: 1.8675\n",
      " Step [10001/479987], Loss: 1.8535\n",
      " Step [10201/479987], Loss: 1.8773\n",
      " Step [10401/479987], Loss: 1.8451\n",
      " Step [10601/479987], Loss: 1.8661\n",
      " Step [10801/479987], Loss: 1.8635\n",
      " Step [11001/479987], Loss: 1.8482\n",
      " Step [11201/479987], Loss: 1.8654\n",
      " Step [11401/479987], Loss: 1.8684\n",
      " Step [11601/479987], Loss: 1.8600\n",
      " Step [11801/479987], Loss: 1.8493\n",
      " Step [12001/479987], Loss: 1.8666\n",
      " Step [12201/479987], Loss: 1.8609\n",
      " Step [12401/479987], Loss: 1.8496\n",
      " Step [12601/479987], Loss: 1.8597\n",
      " Step [12801/479987], Loss: 1.8510\n",
      " Step [13001/479987], Loss: 1.8482\n",
      " Step [13201/479987], Loss: 1.8622\n",
      " Step [13401/479987], Loss: 1.8700\n",
      " Step [13601/479987], Loss: 1.8579\n",
      " Step [13801/479987], Loss: 1.8665\n",
      " Step [14001/479987], Loss: 1.8412\n",
      " Step [14201/479987], Loss: 1.8583\n",
      " Step [14401/479987], Loss: 1.8598\n",
      " Step [14601/479987], Loss: 1.8487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [206], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mnon_scalar_loss(pos_score, neg_score, learning_rate, weight_decay)\n\u001b[1;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 32\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     33\u001b[0m loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m step_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m i\u001b[39m>\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/werk/word_2_vec/venv_wordvec/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/werk/word_2_vec/venv_wordvec/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/werk/word_2_vec/venv_wordvec/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/werk/word_2_vec/venv_wordvec/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/werk/word_2_vec/venv_wordvec/lib/python3.10/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "# optimizer = torch.optim.SparseAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_sum = 0\n",
    "\n",
    "negative_sample_length = 2\n",
    "\n",
    "pos_u_data = torch.ones(batch_size)\n",
    "neg_v_data = torch.zeros(batch_size*negative_sample_length)\n",
    "concat_data = torch.cat([pos_u_data, neg_v_data], dim=0)\n",
    "step_interval = 200\n",
    "weight_decay = 0.0001\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    for i, val in enumerate(train_loader):\n",
    "\n",
    "        pos_u = torch.tensor(val[:,0])\n",
    "        pos_v = torch.tensor(val[:,1])\n",
    "        neg_v = torch.randint(0, dictionary_length, (batch_size, negative_sample_length))\n",
    "        optimizer.zero_grad()\n",
    "        # pass in batch of pos u and pos v\n",
    "        # print(neg_v.shape)\n",
    "        # print(val.shape)\n",
    "\n",
    "        pos_score, neg_score = model(pos_u, pos_v, neg_v)\n",
    "        \n",
    "        loss =model.non_scalar_loss(pos_score, neg_score, learning_rate, weight_decay)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        if i % step_interval == 0 and i>1:\n",
    "            print(' Step [{}/{}], Loss: {:.4f}' \n",
    "                    .format(i+1, len(dataset)//batch_size, loss_sum/step_interval))\n",
    "            loss_sum = 0\n",
    "        if i > len(train_loader) - batch_size:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b8685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a2d4965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_vector(vector1,vector2):\n",
    "    return get_emb(vector1) - get_emb(vector2)\n",
    "\n",
    "def add_vector(vector1,vector2):\n",
    "    return get_emb(vector1) + get_emb(vector2)\n",
    "\n",
    "def cos_sim(vector1, vector2):\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def cos_sim_word(word1, word2):\n",
    "    vector1 = get_emb(word1)\n",
    "    vector2 = get_emb(word2)\n",
    "    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "def get_emb(word):\n",
    "    return model.get_embedding_from_word(word)\n",
    "\n",
    "def invert_dictionary(dictionary):\n",
    "    return {v: k for k, v in dictionary.items()}\n",
    "\n",
    "def get_closest_vector(vector):\n",
    "    max = 0\n",
    "    target = None\n",
    "    for key,item in unique_dict.items():\n",
    "        comparative = get_emb(key)\n",
    "        comparison = cos_sim(vector, comparative)\n",
    "        if comparison > max:\n",
    "            max = comparison\n",
    "            target = key\n",
    "\n",
    "        \n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e70f99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = subtract_vector(\"king\", \"man\")\n",
    "vector = vector +get_emb(\"woman\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "37b692c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1443536 ('flower', 'rose')\n",
      "-0.0018065075 ('flower', 'tree')\n",
      "-0.051516347 ('flower', 'dog')\n",
      "-0.12853625 ('flower', 'cat')\n",
      "-0.09384106 ('flower', 'car')\n",
      "0.094177715 ('cat', 'dog')\n",
      "0.08185257 ('king', 'queen')\n",
      "-0.081511214 ('king', 'royalty')\n",
      "0.42667183 ('queen', 'royalty')\n",
      "-0.045385525 ('man', 'king')\n",
      "0.0038505516 ('woman', 'king')\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim_word(\"flower\", \"rose\"),(\"flower\", \"rose\"))\n",
    "print(cos_sim_word(\"flower\", \"tree\"), (\"flower\", \"tree\"))\n",
    "print(cos_sim_word(\"flower\", \"dog\"), (\"flower\", \"dog\"))\n",
    "print(cos_sim_word(\"flower\", \"cat\"), (\"flower\", \"cat\"))\n",
    "print(cos_sim_word(\"flower\", \"car\"), (\"flower\", \"car\"))\n",
    "print(cos_sim_word(\"cat\", \"dog\"), (\"cat\", \"dog\"))\n",
    "print(cos_sim_word(\"king\", \"queen\"), (\"king\", \"queen\"))\n",
    "print(cos_sim_word(\"king\", \"royalty\"), (\"king\", \"royalty\"))\n",
    "print(cos_sim_word(\"queen\", \"royalty\"), (\"queen\", \"royalty\"))\n",
    "print(cos_sim_word(\"man\", \"king\"), (\"man\", \"king\"))\n",
    "print(cos_sim_word(\"woman\", \"king\"), (\"woman\", \"king\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0fc74988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4999, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_unique_dict = invert_dictionary(unique_dict)\n",
    "model.forward_without_negatives(\"king\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a306e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "index1 = unique_dict[\"king\"]\n",
    "index2 = unique_dict[\"man\"]\n",
    "vector = subtract_vector(\"king\", \"man\")\n",
    "vector = vector+ get_emb(\"woman\")\n",
    "print(get_closest_vector(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "path = \"embeddings\"\n",
    "model.save_embedding\n",
    "model.save_embedding(reversed_unique_dict, \"embeddings.emb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.u_embeddings.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaadf5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wordvec",
   "language": "python",
   "name": "venv_wordvec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b6a9f868315655f86619a29ce0a043959ee2de2105a58d548d1647254ae6395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
